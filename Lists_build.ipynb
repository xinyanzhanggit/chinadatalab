{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhanlp import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import *\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import CoherenceModel\n",
    "import pickle\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in models and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_nums = [10,20,30,40,50,75,100,150,200,300]\n",
    "\n",
    "topic_nums = [10,50]\n",
    "# load LDA models\n",
    "\n",
    "LDAmodels = {}\n",
    "for i in topic_nums:\n",
    "    LDAmodels[i]= models.LdaModel.load('topic_models_without_places/topic_bow_train{}'.format(i))\n",
    "# load corpus 15:20\n",
    "with open('corpus_fifteen.dms', 'rb') as f:\n",
    "    corpus_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents to Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One list for each document\n",
    "# crete an empty list to fill in by lists of each document, representing the network from doc to topic\n",
    "news_topics = []\n",
    "# loop each document\n",
    "for c in corpus_list:\n",
    "    # crete an empty list to fill in by dictionaries of all topics for this document\n",
    "    list_c = []\n",
    "    # loop models\n",
    "    for i in topic_nums:\n",
    "        model = LDAmodels[i]\n",
    "        # loop topic proportions\n",
    "        for r in model[c]:\n",
    "            d = {}\n",
    "            d['Model'] = i\n",
    "            d['TopicIndex'] = str(i) + '_' + str(r[0])\n",
    "            d['TopicName'] = model.print_topics(num_topics = i, num_words = 3)[r[0]][1]\n",
    "            d['TopicProportion'] = r[1]\n",
    "            list_c.append(d)\n",
    "    news_topics.append(list_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list2_doc_topic_list.pickle', 'wb') as f:\n",
    "    pickle.dump(news_topics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_topic = []\n",
    "for i in topic_nums: \n",
    "    model = LDAmodels[i] # 某一个model，比如model30\n",
    "    for topic in range(i):\n",
    "        d = {}\n",
    "        d['Model'] = i\n",
    "        d['TopicIndex'] = str(i) + '_' + str(topic)\n",
    "        d['TopicName'] = model.print_topics(num_topics =topic+1,num_words = 3)[topic][1]\n",
    "        list_topic.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list4_topic.pickle', 'wb') as f:\n",
    "    pickle.dump(list_topic, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics to Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity we choose to use in network    \n",
    "words = ['ni','nic','nit','nr','nrf','ns','nsf','nt','ntc','ntcb','ntcf','ntch',\n",
    "         'nth','nto','nts','ntu']\n",
    "\n",
    "meaning = {'ni':'organization', 'nic': 'subordinate organization', \n",
    "           'nit': 'educational institution', 'nr': 'person',\n",
    "           'nrf': 'person', 'ns':'place', 'nsf':'place', 'nt': 'organization',\n",
    "           'ntc':'company','ntcb':'bank','ntcf':'factory','ntch':'hotel',\n",
    "           'nth':'hospital','nto':'government','nts':'middle and primary school',\n",
    "           'ntu':'university'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:05.934958\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "#run_function():\n",
    "# One list for each topic\n",
    "# crete an empty list to fill in by lists of topics, representing the network from topic to word\n",
    "topics_words = []\n",
    "# loop each model\n",
    "for i in topic_nums:\n",
    "    model = LDAmodels[i]\n",
    "    # loop each topic:\n",
    "    for j in range(i):\n",
    "        word_list = []\n",
    "        key_words = model.show_topics(formatted=False, num_topics = i, num_words = 10)[j][1]\n",
    "        for k in key_words:\n",
    "            pos = HanLP.segment(k[0]).toString().split('/')[1].replace(']', '')\n",
    "            if pos in words:\n",
    "                d = {}\n",
    "                d['Model'] = i\n",
    "                d['TopicIndex'] = str(i) + '_' + str(j)\n",
    "                d['WordName'] = k[0]\n",
    "                d['WordType'] = meaning[pos]\n",
    "                d['WordWeight'] = k[1]\n",
    "                word_list.append(d)\n",
    "        topics_words.append(word_list)\n",
    "end = datetime.datetime.now()\n",
    "print (end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list3_topic_word_list.pickle', 'wb') as f:\n",
    "    pickle.dump(topics_words, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  News to words(with or without noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "path = 'chinese-newspaper-data/trade-news.csv'\n",
    "whole_dateset = pd.read_csv(path)\n",
    "dataset = whole_dateset[15:20]\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Space\n",
    "content_nospace = [cont.replace(' ','') for cont in dataset['content']]\n",
    "# resegmentation, result is a list of words with their POS tagging for each document\n",
    "content_seg = [str(HanLP.segment(cont)).split(', ') for cont in content_nospace]\n",
    "# futher cleaning (delete '[' and ']' )\n",
    "content_seg = [[cont1.replace('[','').replace(']','') for cont1 in cont] for cont in content_seg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity we choose to use in network    \n",
    "words = ['ni','nic','nit','nr','nrf','ns','nsf','nt','ntc','ntcb','ntcf','ntch',\n",
    "         'nth','nto','nts','ntu','n']\n",
    "\n",
    "meaning = {'ni':'organization', 'nic': 'subordinate organization', \n",
    "           'nit': 'educational institution', 'nr': 'person',\n",
    "           'nrf': 'person', 'ns':'place', 'nsf':'place', 'nt': 'organization',\n",
    "           'ntc':'company','ntcb':'bank','ntcf':'factory','ntch':'hotel',\n",
    "           'nth':'hospital','nto':'government','nts':'middle and primary school',\n",
    "           'ntu':'university','n':'noun'}\n",
    "\n",
    "def name_entity(arr):\n",
    "        word_list = []\n",
    "        ner = words \n",
    "        for x in arr:\n",
    "            result = {}\n",
    "            temp = x.split(\"/\")\n",
    "            if(temp[1] in ner):\n",
    "                result['WordName'] = temp[0]\n",
    "                result['WordType'] =  meaning[temp[1]]\n",
    "                word_list.append(result)\n",
    "        return word_list\n",
    "\n",
    "news_words_list = [name_entity(set(doc)) for doc in content_seg] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list5_news_word_list.pickle', 'wb') as f:\n",
    "    pickle.dump(news_words_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'NewsName': '“文化中国·四海同春”精彩亮相', 'newspaper': '甘肃日报', 'date': '2012-01-31'},\n",
       " {'NewsName': '巴基斯坦总理吉拉尼称本国政治危机已经缓解',\n",
       "  'newspaper': '甘肃日报',\n",
       "  'date': '2012-01-31'},\n",
       " {'NewsName': '伊朗称“不久”将停止向欧洲某些国家供油',\n",
       "  'newspaper': '甘肃日报',\n",
       "  'date': '2012-01-31'},\n",
       " {'NewsName': '讨论《政府工作报告（征求意见稿）》', 'newspaper': '甘肃日报', 'date': '2012-02-01'},\n",
       " {'NewsName': '欧盟25国通过“财政契约”草案', 'newspaper': '甘肃日报', 'date': '2012-02-01'}]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of dictionary: list of news. List #1\n",
    "list_news = []\n",
    "for i in range(len(dataset)):\n",
    "    list_news.append( {'NewsName':dataset.title[i],\\\n",
    "                       'newspaper':dataset.newspaper[i],\\\n",
    "                       'date':dataset.date[i]})\n",
    "    # news_list\n",
    "list_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list1_news.pickle', 'wb') as f:\n",
    "    pickle.dump(list_news, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
