{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhanlp import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import *\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import CoherenceModel\n",
    "import pickle\n",
    "from googletrans import Translator\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in models and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_nums = [10,20,30,40,50,75,100,150,200,300]\n",
    "\n",
    "topic_nums = [10,50]\n",
    "# load LDA models\n",
    "\n",
    "LDAmodels = {}\n",
    "for i in topic_nums:\n",
    "    LDAmodels[i]= models.LdaModel.load('topic_10_50_with_POS/topic_bow_train{}'.format(i))\n",
    "# load corpus 15:20\n",
    "with open('corpus_fifteen.dms', 'rb') as f:\n",
    "    corpus_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get word and wordtype\n",
    "def get_word_wordtype(word_and_type):\n",
    "    words = ['ni','nic','nit','nr','nrf','ns','nsf','nt','ntc','ntcb','ntcf','ntch',\n",
    "         'nth','nto','nts','ntu','n']\n",
    "\n",
    "    meaning = {'ni':'organization', 'nic': 'subordinate organization', \n",
    "           'nit': 'educational institution', 'nr': 'person',\n",
    "           'nrf': 'person', 'ns':'place', 'nsf':'place', 'nt': 'organization',\n",
    "           'ntc':'company','ntcb':'bank','ntcf':'factory','ntch':'hotel',\n",
    "           'nth':'hospital','nto':'government','nts':'middle and primary school',\n",
    "           'ntu':'university','n':'noun'}\n",
    "    word = word_and_type.split('/')[0]\n",
    "    word_type = meaning[word_and_type.split('/')[1]]\n",
    "    return(word,word_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get topic name: 0.145*\"经济/n\" + 0.032*\"我国/n\" + 0.022*\"企业/n\"\n",
    "def get_topic_name(long_topic_name):\n",
    "    topic_name = re.sub('[a-zA-Z0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?、…【】《》？“”‘’！[\\\\]^_`{|}~\\s]+', \" \", \\\n",
    "                        long_topic_name).strip()\n",
    "    return topic_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents to Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One list for each document\n",
    "# crete an empty list to fill in by lists of each document, representing the network from doc to topic\n",
    "news_topics = []\n",
    "# loop each document\n",
    "for c in corpus_list:\n",
    "    # crete an empty list to fill in by dictionaries of all topics for this document\n",
    "    list_c = []\n",
    "    # loop models\n",
    "    for i in topic_nums:\n",
    "        model = LDAmodels[i]\n",
    "        # loop topic proportions\n",
    "        for r in model[c]:\n",
    "            d = {}\n",
    "            d['Model'] = i\n",
    "            d['TopicIndex'] = str(i) +  '_' +str(r[0])\n",
    "            d['TopicName'] = get_topic_name(model.print_topics(num_topics = i, num_words = 3)[r[0]][1])\n",
    "            d['TopicProportion'] = r[1]\n",
    "            list_c.append(d)\n",
    "    news_topics.append(list_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list2_doc_topic_list.pickle', 'wb') as f:\n",
    "    pickle.dump(news_topics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Model': 10,\n",
       "  'TopicIndex': '10_3',\n",
       "  'TopicName': '产业 企业 项目',\n",
       "  'TopicProportion': 0.9525966},\n",
       " {'Model': 50,\n",
       "  'TopicIndex': '50_9',\n",
       "  'TopicName': '省 我省 全省',\n",
       "  'TopicProportion': 0.1527766},\n",
       " {'Model': 50,\n",
       "  'TopicIndex': '50_12',\n",
       "  'TopicName': '规划 产业 新区',\n",
       "  'TopicProportion': 0.33370557},\n",
       " {'Model': 50,\n",
       "  'TopicIndex': '50_18',\n",
       "  'TopicName': '产业 科技 人才',\n",
       "  'TopicProportion': 0.27084327},\n",
       " {'Model': 50,\n",
       "  'TopicIndex': '50_36',\n",
       "  'TopicName': '城市 国际 市',\n",
       "  'TopicProportion': 0.1942375}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_topics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_topic = []\n",
    "for i in topic_nums: \n",
    "    model = LDAmodels[i] # 某一个model，比如model30\n",
    "    for topic in range(i):\n",
    "        d = {}\n",
    "        d['Model'] = i\n",
    "        d['TopicIndex'] = str(i) + '_' + str(topic)\n",
    "        d['TopicName'] = get_topic_name(model.print_topics(num_topics =topic+1,num_words = 3)[topic][1])\n",
    "        list_topic.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list4_topic.pickle', 'wb') as f:\n",
    "    pickle.dump(list_topic, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics to Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:09.090820\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "#run_function():\n",
    "# One list for each topic\n",
    "# crete an empty list to fill in by lists of topics, representing the network from topic to word\n",
    "topics_words = []\n",
    "# loop each model\n",
    "for i in topic_nums:\n",
    "    model = LDAmodels[i]\n",
    "    # loop each topic:\n",
    "    for j in range(i):\n",
    "        word_list = []\n",
    "        key_words = model.show_topics(formatted=False, num_topics = i, num_words = 10)[j][1]\n",
    "        for k in key_words:\n",
    "            d = {}\n",
    "            d['Model'] = i\n",
    "            d['TopicIndex'] = str(i) + '_' + str(j)\n",
    "            d['WordName'] = get_word_wordtype(k[0])[0]\n",
    "            d['WordType'] = get_word_wordtype(k[0])[1]\n",
    "            d['WordWeight'] = k[1]             \n",
    "            word_list.append(d)\n",
    "        topics_words.append(word_list)\n",
    "end = datetime.datetime.now()\n",
    "print (end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list3_topic_word_list.pickle', 'wb') as f:\n",
    "    pickle.dump(topics_words, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  News to words(with or without noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "path = 'chinese-newspaper-data/trade-news.csv'\n",
    "whole_dateset = pd.read_csv(path)\n",
    "dataset = whole_dateset[15:20]\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    # Delete Space and truncate the text \n",
    "    text = text.replace(' ','')\n",
    "    if len(text)>50:\n",
    "        text = text[20:-20]\n",
    "    # resegmentation, result is a list of words with their POS tagging for each document\n",
    "    text = str(HanLP.segment(text)).replace('[','').replace(']','').split(', ')\n",
    "    return text\n",
    "\n",
    "content_seg = list(map(text_preprocess, dataset['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity we choose to use in network    \n",
    "words = ['ni','nic','nit','nr','nrf','ns','nsf','nt','ntc','ntcb','ntcf','ntch',\n",
    "         'nth','nto','nts','ntu','n']\n",
    "\n",
    "meaning = {'ni':'organization', 'nic': 'subordinate organization', \n",
    "           'nit': 'educational institution', 'nr': 'person',\n",
    "           'nrf': 'person', 'ns':'place', 'nsf':'place', 'nt': 'organization',\n",
    "           'ntc':'company','ntcb':'bank','ntcf':'factory','ntch':'hotel',\n",
    "           'nth':'hospital','nto':'government','nts':'middle and primary school',\n",
    "           'ntu':'university','n':'noun'}\n",
    "\n",
    "def name_entity(arr):\n",
    "        word_list = []\n",
    "        ner = words \n",
    "        for x in arr:\n",
    "            result = {}\n",
    "            temp = x.split(\"/\")\n",
    "            if(temp[1] in ner):\n",
    "                result['WordName'] = temp[0]\n",
    "                result['WordType'] =  meaning[temp[1]]\n",
    "                word_list.append(result)\n",
    "        return word_list\n",
    "\n",
    "news_words_list = [name_entity(set(doc)) for doc in content_seg] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list5_news_word_list.pickle', 'wb') as f:\n",
    "    pickle.dump(news_words_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'NewsName': '“文化中国·四海同春”精彩亮相', 'newspaper': '甘肃日报', 'date': '2012-01-31'},\n",
       " {'NewsName': '巴基斯坦总理吉拉尼称本国政治危机已经缓解',\n",
       "  'newspaper': '甘肃日报',\n",
       "  'date': '2012-01-31'},\n",
       " {'NewsName': '伊朗称“不久”将停止向欧洲某些国家供油',\n",
       "  'newspaper': '甘肃日报',\n",
       "  'date': '2012-01-31'},\n",
       " {'NewsName': '讨论《政府工作报告（征求意见稿）》', 'newspaper': '甘肃日报', 'date': '2012-02-01'},\n",
       " {'NewsName': '欧盟25国通过“财政契约”草案', 'newspaper': '甘肃日报', 'date': '2012-02-01'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of dictionary: list of news. List #1\n",
    "list_news = []\n",
    "for i in range(len(dataset)):\n",
    "    list_news.append( {'NewsName':dataset.title[i],\\\n",
    "                       'newspaper':dataset.newspaper[i],\\\n",
    "                       'date':dataset.date[i]})\n",
    "    # news_list\n",
    "list_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list1_news.pickle', 'wb') as f:\n",
    "    pickle.dump(list_news, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
