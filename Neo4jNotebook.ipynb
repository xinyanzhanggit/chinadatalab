{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx  as nx\n",
    "import pandas as pd\n",
    "from pyhanlp import *\n",
    "from itertools import combinations as comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "path = 'chinese-newspaper-data/trade-news.csv'\n",
    "whole_dateset = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = whole_dateset[15:20]\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Space\n",
    "content_nospace = [cont.replace(' ','') for cont in dataset['content']]\n",
    "# resegmentation, result is a list of words with their POS tagging for each document\n",
    "content_seg = [str(HanLP.segment(cont)).split(', ') for cont in content_nospace]\n",
    "# futher cleaning (delete '[' and ']' )\n",
    "content_seg = [[cont1.replace('[','').replace(']','') for cont1 in cont] for cont in content_seg]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract word New version: Separate Different Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity we choose to use in network    \n",
    "words = ['ni','nic','nit','nr','nrf','ns','nsf','nt','ntc','ntcb','ntcf','ntch',\n",
    "         'nth','nto','nts','ntu']\n",
    "\n",
    "meaning = {'ni':'organization', 'nic': 'subordinate organization', \n",
    "           'nit': 'educational institution', 'nr': 'person',\n",
    "           'nrf': 'person', 'ns':'place', 'nsf':'place', 'nt': 'organization',\n",
    "           'ntc':'company','ntcb':'bank','ntcf':'factory','ntch':'hotel',\n",
    "           'nth':'hospital','nto':'government','nts':'middle and primary school',\n",
    "           'ntu':'university'}\n",
    "\n",
    "# extract words we want according to it's POS tagging\n",
    "def get_words_and_POS(arr):\n",
    "        result = {}\n",
    "        ner = words \n",
    "        for x in arr:\n",
    "            temp = x.split(\"/\")\n",
    "            if(temp[1] in ner):\n",
    "                result[temp[0]] = meaning[temp[1]]\n",
    "        return result\n",
    "\n",
    "# filtered list of words for each document\n",
    "words_list = [get_words_and_POS(doc) for doc in content_seg]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to neo4j and build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install py2neo\n",
    "from py2neo import Graph,Node,Relationship,PropertyDict,Subgraph,NodeMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = []\n",
    "for i in range(len(dataset)):\n",
    "    news_list.append( {'NewsName':str(i),'newspaper':dataset.newspaper[i],'date':dataset.date[i]}   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph('http://localhost:7474',username='neo4j',password='test')\n",
    "\n",
    "g.delete_all()\n",
    "tx = g.begin()\n",
    "\n",
    "for i in range(len(news_list)):\n",
    "    news = news_list[i]\n",
    "    news_node = Node(\"News\",**news)\n",
    "    news_node.__primarylabel__ = \"News\"\n",
    "    news_node.__primarykey__ = \"NewsName\"\n",
    "    tx.merge(news_node)\n",
    "    \n",
    "    for word in words_list[i]:\n",
    "        type_ = words_list[i][word]\n",
    "        \n",
    "        words_node = Node(type_,WordName=word)\n",
    "        words_node.__primarylabel__ = type_\n",
    "        words_node.__primarykey__ = \"WordName\"    \n",
    "        tx.merge(words_node)\n",
    "        \n",
    "        Has = Relationship.type(\"has\")\n",
    "        tx.merge(Has(news_node,words_node))\n",
    "    \n",
    "tx.commit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic: Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic data from Tony's file\n",
    "topic_data = pd.read_pickle(\"topic_50_words.dms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change topic data to proper format (List of dictionary)\n",
    "topic_word_entity = []\n",
    "\n",
    "for topic in topic_data:\n",
    "    topic_word = []\n",
    "    for word in topic:\n",
    "        word_entitiy = str(HanLP.segment(word)).replace('[','').replace(']','') \n",
    "        topic_word.append(word_entitiy)\n",
    "    topic_word_entity.append(topic_word)\n",
    "    \n",
    "def get_words_and_POS(arr):\n",
    "        result = {}\n",
    "        ner = words \n",
    "        for x in arr:\n",
    "            temp = x.split(\"/\")\n",
    "            if(temp[1] in ner):\n",
    "                result[temp[0]] = meaning[temp[1]]\n",
    "        return result\n",
    "    \n",
    "topic_word_entity_list = [get_words_and_POS(doc) for doc in topic_word_entity]\n",
    "#topic_word_entity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = []\n",
    "for i in range(len(topic_data)):\n",
    "    topic_list.append( {'TopicName':'Topic '+ str(i)}) # We can add the inference of Topic like the topic nick name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic: Build neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph('http://localhost:7474',username='neo4j',password='test')\n",
    "\n",
    "tx = g.begin()\n",
    "\n",
    "for i in range(len(topic_list)):\n",
    "    topics = topic_list[i]\n",
    "    topics_node = Node(\"Topics\",**topics)\n",
    "    topics_node.__primarylabel__ = \"Topics\"\n",
    "    topics_node.__primarykey__ = \"TopicName\"\n",
    "    tx.merge(topics_node)\n",
    "    \n",
    "    for word in topic_word_entity_list[i]:\n",
    "        type_ = topic_word_entity_list[i][word]\n",
    "        \n",
    "        words_node = Node(type_,WordName=word)\n",
    "        words_node.__primarylabel__ = type_\n",
    "        words_node.__primarykey__ = \"WordName\"    \n",
    "        tx.merge(words_node)\n",
    "        \n",
    "        Has = Relationship.type(\"contains\")\n",
    "        tx.merge(Has(topics_node,words_node))\n",
    "    \n",
    "tx.commit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
