{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhanlp import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import *\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import CoherenceModel\n",
    "import pickle\n",
    "from googletrans import Translator\n",
    "import re\n",
    "# pip install py2neo\n",
    "from py2neo import Graph,Node,Relationship,PropertyDict,Subgraph,NodeMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in dataset, models and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use 5 documents to test the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "path = 'chinese-newspaper-data/trade-news.csv'\n",
    "whole_dateset = pd.read_csv(path)\n",
    "dataset = whole_dateset[15:20]\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models from the 'topic_10_50_with_POS' file. \n",
    "topic_nums = [10,50]\n",
    "LDAmodels = {}\n",
    "for i in topic_nums:\n",
    "    LDAmodels[i]= models.LdaModel.load('topic_10_50_with_POS/topic_bow_train{}'.format(i))\n",
    "# load corpus 15:20\n",
    "with open('corpus_fifteen.dms', 'rb') as f:\n",
    "    corpus_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build function to get word, word type, change topic name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get word and wordtype\n",
    "def get_word_wordtype(word_and_type):\n",
    "    words = ['ni','nic','nit','nr','nrf','ns','nsf','nt','ntc','ntcb','ntcf','ntch',\n",
    "         'nth','nto','nts','ntu','n']\n",
    "\n",
    "    meaning = {'ni':'organization', 'nic': 'subordinate organization', \n",
    "           'nit': 'educational institution', 'nr': 'person',\n",
    "           'nrf': 'person', 'ns':'place', 'nsf':'place', 'nt': 'organization',\n",
    "           'ntc':'company','ntcb':'bank','ntcf':'factory','ntch':'hotel',\n",
    "           'nth':'hospital','nto':'government','nts':'middle and primary school',\n",
    "           'ntu':'university','n':'noun'}\n",
    "    word = word_and_type.split('/')[0]\n",
    "    word_type = meaning[word_and_type.split('/')[1]]\n",
    "    return(word,word_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get topic name: ‘0.145*\"经济/n\" + 0.032*\"我国/n\" + 0.022*\"企业/n\" ' to '经济 我国 企业'\n",
    "def get_topic_name(long_topic_name):\n",
    "    topic_name = re.sub('[a-zA-Z0-9’!\"#$%&\\'()*+,-./:;<=>?@，。?、…【】《》？“”‘’！[\\\\]^_`{|}~\\s]+', \" \", \\\n",
    "                        long_topic_name).strip()\n",
    "    return topic_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List #1: list of dictionary: list of news. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list is made of many dictionaries, which contain the informaction of each news. \n",
    "\n",
    "It is created to create News Node(NewsName,Date, Newspaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#大的list是由很多的dictionary组成的,每个dictionary是news的信息\n",
    "#用来建立News Node(NewsName,Date,Newspaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of dictionary: list of news. List #1\n",
    "list_news = []\n",
    "for i in range(len(dataset)):\n",
    "    list_news.append( {'NewsName':dataset.title[i],\\\n",
    "                       'newspaper':dataset.newspaper[i],\\\n",
    "                       'date':dataset.date[i]})\n",
    "    # news_list\n",
    "list_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list1_news.pickle', 'wb') as f:\n",
    "    pickle.dump(list_news, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List #2 list of list of dictionary: list of news, list of topic and news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list is made of many lists which is the information of a news. Each news list is made of many dictionaries. Each dictionary is the topics covered by this news. \n",
    "\n",
    "It includes not only the topics from one topic model, for example, topic model with 10 topics, but includes the topics from other topic model for example, topic model with 50 topics as well.\n",
    "\n",
    "This list is used to build Topic Node(Model,TopicIndex,TopicName) and News_Covers_Topic Relationship（TopicProportion）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#大的list是由很多list组成的，\n",
    "#每一个list是一个news，这个list是由很多dictionary组成的，每个dictionary是这个news包含的topic,\n",
    "#不光包括一个model的topic，包括所有model的topic\n",
    "#比如topic10_1,topic50_5\n",
    "#顺序是model从少到多，index从小到大\n",
    "\n",
    "#用来建立 Topic Node(Model,TopicIndex,TopicName)\n",
    "#用来建立 News_Covers_Topic Relationship（TopicProportion）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One list for each document\n",
    "# crete an empty list to fill in by lists of each document, representing the network from doc to topic\n",
    "news_topics = []\n",
    "# loop each document\n",
    "for c in corpus_list:\n",
    "    # crete an empty list to fill in by dictionaries of all topics for this document\n",
    "    list_c = []\n",
    "    # loop models\n",
    "    for i in topic_nums:\n",
    "        model = LDAmodels[i]\n",
    "        # loop topic proportions\n",
    "        for r in model[c]:\n",
    "            d = {}\n",
    "            d['Model'] = i\n",
    "            d['TopicIndex'] = str(i) +  '_' +str(r[0])\n",
    "            d['TopicName'] = get_topic_name(model.print_topics(num_topics = i, num_words = 3)[r[0]][1])\n",
    "            d['TopicProportion'] = r[1]\n",
    "            list_c.append(d)\n",
    "    news_topics.append(list_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list2_doc_topic_list.pickle', 'wb') as f:\n",
    "    pickle.dump(news_topics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Model': 10,\n",
       "  'TopicIndex': '10_3',\n",
       "  'TopicName': '产业 企业 项目',\n",
       "  'TopicProportion': 0.9525966},\n",
       " {'Model': 50,\n",
       "  'TopicIndex': '50_9',\n",
       "  'TopicName': '省 我省 全省',\n",
       "  'TopicProportion': 0.1527766},\n",
       " {'Model': 50,\n",
       "  'TopicIndex': '50_12',\n",
       "  'TopicName': '规划 产业 新区',\n",
       "  'TopicProportion': 0.33370557},\n",
       " {'Model': 50,\n",
       "  'TopicIndex': '50_18',\n",
       "  'TopicName': '产业 科技 人才',\n",
       "  'TopicProportion': 0.27084327},\n",
       " {'Model': 50,\n",
       "  'TopicIndex': '50_36',\n",
       "  'TopicName': '城市 国际 市',\n",
       "  'TopicProportion': 0.1942375}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_topics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List #3:  list of list of dictionary: list of topic, list of topic and words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list is made of many lists which is the information of a topic. Each topic list is made of many dictionaries. Each dictionary is the information of words contained in this topic. The word information includes word name, word type (person/organization/government/company.....), weight of this word with the topic. \n",
    "\n",
    "This list is used to build Word Node (WordName,WordType) and Topics_Contains_Word Relationship(WordWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#大的list是由很多的list组成的\n",
    "#每个list是一个topic的信息，由很多dictionary组成的\n",
    "#topics的顺序是model从少到多，index从小到大\n",
    "#每个dictionary是一个topic中的某一个word的的信息，包括wordname，wordtype\n",
    "#还有wordweight用来建立topic和word之间的relationship\n",
    "\n",
    "#用来建立 Word Node(WordName,WordType)\n",
    "#用来建立 Topics_Contains_Word Relationship(WordWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:09.090820\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "#run_function():\n",
    "# One list for each topic\n",
    "# crete an empty list to fill in by lists of topics, representing the network from topic to word\n",
    "topics_words = []\n",
    "# loop each model\n",
    "for i in topic_nums:\n",
    "    model = LDAmodels[i]\n",
    "    # loop each topic:\n",
    "    for j in range(i):\n",
    "        word_list = []\n",
    "        key_words = model.show_topics(formatted=False, num_topics = i, num_words = 10)[j][1]\n",
    "        for k in key_words:\n",
    "            d = {}\n",
    "            d['Model'] = i\n",
    "            d['TopicIndex'] = str(i) + '_' + str(j)\n",
    "            d['WordName'] = get_word_wordtype(k[0])[0]\n",
    "            d['WordType'] = get_word_wordtype(k[0])[1]\n",
    "            d['WordWeight'] = k[1]             \n",
    "            word_list.append(d)\n",
    "        topics_words.append(word_list)\n",
    "end = datetime.datetime.now()\n",
    "print (end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list3_topic_word_list.pickle', 'wb') as f:\n",
    "    pickle.dump(topics_words, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List #4: list of topic: list of topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list is made of many dictionaries, which contain the informaction of each topic. The information contains model，topicsname，topicindex. \n",
    "\n",
    "It is created to create Topic Node(Model,TopicIndex,TopicName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大的list是由很多的dictionary组成的\n",
    "# 每个dictionary是一个topic的信息，包括model，topicsname，topicindex\n",
    "# topics的顺序是model从少到多，index从小到大\n",
    "# 用来建立 Topic Node(Model,TopicIndex,TopicName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_topic = []\n",
    "for i in topic_nums: \n",
    "    model = LDAmodels[i] # 某一个model，比如model30\n",
    "    for topic in range(i):\n",
    "        d = {}\n",
    "        d['Model'] = i\n",
    "        d['TopicIndex'] = str(i) + '_' + str(topic)\n",
    "        d['TopicName'] = get_topic_name(model.print_topics(num_topics =topic+1,num_words = 3)[topic][1])\n",
    "        list_topic.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list4_topic.pickle', 'wb') as f:\n",
    "    pickle.dump(list_topic, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List #5: list of list of dictionary: list of news, list of word in this news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list is made of many lists which is the information of a news. Each news list is made of many dictionaries. \n",
    "\n",
    "Each dictionary is the information of words contained in this topic. The word information includes word name, word type (person/organization/government/company.....), weight of this word with the topic. \n",
    "\n",
    "This list is used to build Word Node (WordName,WordType) and News_Has_Word Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大的list是由很多的list组成的\n",
    "# 每个list是一个news的信息，由很多dictionary组成的\n",
    "# 每个dictionary是一个news中的某一个word的的信息，包括wordname，wordtype\n",
    "\n",
    "# 用来建立 Word Node（WordName,WordType）\n",
    "# 用来建立 News_Has_Word Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    # Delete Space and truncate the text \n",
    "    text = text.replace(' ','')\n",
    "    if len(text)>50:\n",
    "        text = text[20:-20]\n",
    "    # resegmentation, result is a list of words with their POS tagging for each document\n",
    "    text = str(HanLP.segment(text)).replace('[','').replace(']','').split(', ')\n",
    "    return text\n",
    "\n",
    "content_seg = list(map(text_preprocess, dataset['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity we choose to use in network    \n",
    "words = ['ni','nic','nit','nr','nrf','ns','nsf','nt','ntc','ntcb','ntcf','ntch',\n",
    "         'nth','nto','nts','ntu','n']\n",
    "\n",
    "meaning = {'ni':'organization', 'nic': 'subordinate organization', \n",
    "           'nit': 'educational institution', 'nr': 'person',\n",
    "           'nrf': 'person', 'ns':'place', 'nsf':'place', 'nt': 'organization',\n",
    "           'ntc':'company','ntcb':'bank','ntcf':'factory','ntch':'hotel',\n",
    "           'nth':'hospital','nto':'government','nts':'middle and primary school',\n",
    "           'ntu':'university','n':'noun'}\n",
    "\n",
    "def name_entity(arr):\n",
    "        word_list = []\n",
    "        ner = words \n",
    "        for x in arr:\n",
    "            result = {}\n",
    "            temp = x.split(\"/\")\n",
    "            if(temp[1] in ner):\n",
    "                result['WordName'] = temp[0]\n",
    "                result['WordType'] =  meaning[temp[1]]\n",
    "                word_list.append(result)\n",
    "        return word_list\n",
    "\n",
    "news_words_list = [name_entity(set(doc)) for doc in content_seg] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list5_news_word_list.pickle', 'wb') as f:\n",
    "    pickle.dump(news_words_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load list inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list 1 \n",
    "list_news_topic = pd.read_pickle(\"list1_news.pickle\")\n",
    "# list 2 \n",
    "list_news_topic = pd.read_pickle(\"list2_doc_topic_list.pickle\")\n",
    "# list 3\n",
    "list_topic_word =  pd.read_pickle('list3_topic_word_list.pickle')\n",
    "# list 4\n",
    "list_topic =  pd.read_pickle('list4_topic.pickle')\n",
    "# list 5\n",
    "list_news_word =  pd.read_pickle('list5_news_word_list.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build network using neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph('http://localhost:7474',username='neo4j',password='test')\n",
    "g.delete_all()\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "tx = g.begin()\n",
    "\n",
    "### Build relationship 2: Topic contains word\n",
    "\n",
    "for i in range(len(list_topic)):\n",
    "    topic = list_topic[i]\n",
    "   # Build node of news\n",
    "    topic_node = Node(\"Topic\",**topic)\n",
    "    topic_node.__primarylabel__ = \"Topic\"\n",
    "    topic_node.__primarykey__ = \"TopicIndex\"\n",
    "    tx.merge(topic_node)\n",
    "    \n",
    "    for word in list_topic_word[i]:   \n",
    "        type_ = word['WordType']\n",
    "        weight = word['WordWeight']\n",
    "        # Build node of topic\n",
    "        word_node = Node(type_,WordName=word['WordName'])\n",
    "        word_node.__primarylabel__ = type_\n",
    "        word_node.__primarykey__ = \"WordName\" \n",
    "        tx.merge(word_node)\n",
    "        \n",
    "        # Build topic-word relationship\n",
    "        Topic_Word = Relationship.type(\"Topic_Contains_Word\")\n",
    "        tx.merge(Topic_Word(topic_node,word_node,WordWeight=str(weight)))\n",
    "\n",
    "### Build relationship 1:News covers topic and relationship 3: News_Has_Word\n",
    "\n",
    "for i in range(len(list_news)):\n",
    "    news = list_news[i]\n",
    "   # Build node of news\n",
    "    news_node = Node(\"News\",**news)\n",
    "    news_node.__primarylabel__ = \"News\"\n",
    "    news_node.__primarykey__ = \"NewsName\"\n",
    "    tx.merge(news_node)\n",
    "       \n",
    "    ### Build Relationship 1: News covers topic\n",
    "    \n",
    "    for topic in list_news_topic[i]:   \n",
    "       \n",
    "        # Topic node: only remains attribute: Model,TopicIndex,TopicName\n",
    "        new_topic_node = dict(topic)\n",
    "        del(new_topic_node[('TopicProportion')])\n",
    "        \n",
    "        # Build node of topic\n",
    "        topic_node = Node('Topic',**new_topic_node)\n",
    "        topic_node.__primarylabel__ = 'Topic'\n",
    "        topic_node.__primarykey__ = \"TopicIndex\"    \n",
    "        tx.merge(topic_node)\n",
    "        \n",
    "        # Build topic-news relationship\n",
    "        Topic_News = Relationship.type(\"News_Covers_Topic\")\n",
    "        tx.merge(Topic_News(news_node,topic_node,\\\n",
    "                            TopicProportion=str(topic['TopicProportion'])))\n",
    "        \n",
    "    # Build Relationship 3 : News_Has_Word\n",
    "    \n",
    "    for word in list_news_word[i]:   \n",
    "       \n",
    "        # Build node of word\n",
    "        type_ = word['WordType']\n",
    "        word_node = Node(type_,WordName=word['WordName'])\n",
    "        word_node.__primarylabel__ = type_\n",
    "        word_node.__primarykey__ = \"WordName\"    \n",
    "        tx.merge(word_node)\n",
    "        \n",
    "        # Build word-news relationship\n",
    "        Word_News = Relationship.type(\"News_Has_Word\")\n",
    "        tx.merge(Word_News(news_node,word_node))\n",
    "           \n",
    "tx.commit() \n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print ('Time Spent:')\n",
    "print (end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
